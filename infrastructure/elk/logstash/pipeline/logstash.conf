# Version: 1.0
# This pipeline processes logs from Payment-EAPI, Payment-SAPI, Conjur Vault, and Redis Cache

# Global variables
ELASTIC_USER = "${ELASTIC_USER:elastic}"
ELASTIC_PASSWORD = "${ELASTIC_PASSWORD}"
ENVIRONMENT = "${ENVIRONMENT:production}"

# ================================= Input Section ==================================
input {
  # Receive logs from Filebeat instances
  beats {
    port => 5044
    ssl => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
    ssl_verify_mode => "force_peer"
  }
  
  # Receive logs via TCP for direct application logging
  tcp {
    port => 5000
    codec => json_lines
  }
}

# ================================= Filter Section =================================
filter {
  # Process logs from Payment-EAPI service
  if [fields][type] == "payment-eapi" {
    json {
      source => "message"
      target => "parsed_json"
      skip_on_invalid_json => true
    }
    
    # Parse non-JSON logs if needed
    if ![parsed_json] {
      grok {
        match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:thread}\] \[%{DATA:correlation_id}\] %{LOGLEVEL:log_level} %{DATA:logger} - %{GREEDYDATA:log_message}" }
      }
    }
    
    # Extract authentication events
    grok {
      match => { "log_message" => "Authentication (?<auth_result>success|failure) for client_id=(?<client_id>[^ ]+)" }
      tag_on_failure => ["_auth_extract_failure"]
    }
    
    # Add tags for authentication events
    if [auth_result] {
      mutate {
        add_tag => ["authentication_event", "authentication_%{auth_result}"]
      }
    }
    
    # Remove sensitive data
    mutate {
      gsub => [
        "\"client_secret\":\"[^\"]+\"", "\"client_secret\":\"[REDACTED]\"",
        "\"password\":\"[^\"]+\"", "\"password\":\"[REDACTED]\"",
        "\"token\":\"[^\"]+\"", "\"token\":\"[REDACTED]\"",
        "Bearer [^\s]+", "Bearer [REDACTED]"
      ]
    }
  }
  
  # Process security logs from Payment-EAPI service
  else if [fields][type] == "payment-eapi-security" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:thread}\] \[%{DATA:correlation_id}\] %{LOGLEVEL:log_level} %{DATA:logger} - %{GREEDYDATA:log_message}" }
    }
    
    # Add security tags
    mutate {
      add_tag => ["security_event"]
    }
    
    # Remove sensitive data
    mutate {
      gsub => [
        "\"client_secret\":\"[^\"]+\"", "\"client_secret\":\"[REDACTED]\"",
        "\"password\":\"[^\"]+\"", "\"password\":\"[REDACTED]\"",
        "\"token\":\"[^\"]+\"", "\"token\":\"[REDACTED]\"",
        "Bearer [^\s]+", "Bearer [REDACTED]"
      ]
    }
  }
  
  # Process logs from Payment-SAPI service
  else if [fields][type] == "payment-sapi" {
    json {
      source => "message"
      target => "parsed_json"
      skip_on_invalid_json => true
    }
    
    # Parse non-JSON logs if needed
    if ![parsed_json] {
      grok {
        match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:thread}\] \[%{DATA:correlation_id}\] %{LOGLEVEL:log_level} %{DATA:logger} - %{GREEDYDATA:log_message}" }
      }
    }
    
    # Extract token validation events
    grok {
      match => { "log_message" => "Token validation (?<validation_result>success|failure) for token_id=(?<token_id>[^ ]+)" }
      tag_on_failure => ["_token_extract_failure"]
    }
    
    # Add tags for token validation events
    if [validation_result] {
      mutate {
        add_tag => ["token_validation_event", "token_validation_%{validation_result}"]
      }
    }
    
    # Remove sensitive data
    mutate {
      gsub => [
        "\"token\":\"[^\"]+\"", "\"token\":\"[REDACTED]\"",
        "Bearer [^\s]+", "Bearer [REDACTED]"
      ]
    }
  }
  
  # Process security logs from Payment-SAPI service
  else if [fields][type] == "payment-sapi-security" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:thread}\] \[%{DATA:correlation_id}\] %{LOGLEVEL:log_level} %{DATA:logger} - %{GREEDYDATA:log_message}" }
    }
    
    # Add security tags
    mutate {
      add_tag => ["security_event"]
    }
    
    # Remove sensitive data
    mutate {
      gsub => [
        "\"token\":\"[^\"]+\"", "\"token\":\"[REDACTED]\"",
        "Bearer [^\s]+", "Bearer [REDACTED]"
      ]
    }
  }
  
  # Process logs from Conjur vault
  else if [fields][type] == "conjur-vault" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} %{DATA:facility} %{GREEDYDATA:log_message}" }
    }
    
    # Add tags for credential events
    if "credential" in [log_message] {
      mutate {
        add_tag => ["credential_event"]
      }
    }
    
    # Remove sensitive data
    mutate {
      gsub => [
        "\"api_key\":\"[^\"]+\"", "\"api_key\":\"[REDACTED]\"",
        "\"secret\":\"[^\"]+\"", "\"secret\":\"[REDACTED]\""
      ]
    }
  }
  
  # Process logs from Redis cache
  else if [fields][type] == "redis-cache" {
    grok {
      match => { "message" => "%{REDISTIMESTAMP:timestamp} %{REDISLEVEL:log_level} %{GREEDYDATA:log_message}" }
      patterns_dir => ["/etc/logstash/patterns"]
    }
    
    # Add tags for cache events
    mutate {
      add_tag => ["cache_event"]
    }
  }
  
  # Common processing for all logs
  # Set @timestamp from log timestamp
  date {
    match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSS"]
    target => "@timestamp"
    timezone => "UTC"
  }
  
  # Add environment field
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:production}"
    }
  }
  
  # Remove fields that are no longer needed
  mutate {
    remove_field => ["timestamp", "parsed_json"]
  }
}

# ================================= Output Section =================================
output {
  # Send processed logs to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    user => "${ELASTIC_USER:elastic}"
    password => "${ELASTIC_PASSWORD}"
    index => "%{[fields][type]}-%{+YYYY.MM.dd}"
    ssl => true
    ssl_certificate_verification => true
    ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
    template_overwrite => true
    ilm_enabled => true
    ilm_rollover_alias => "payment-logs"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "payment-logs-policy"
  }
  
  # Send security logs to a separate security index
  if "security_event" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      user => "${ELASTIC_USER:elastic}"
      password => "${ELASTIC_PASSWORD}"
      index => "security-events-%{+YYYY.MM.dd}"
      ssl => true
      ssl_certificate_verification => true
      ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
      template_overwrite => true
      ilm_enabled => true
      ilm_rollover_alias => "security-logs"
      ilm_pattern => "{now/d}-000001"
      ilm_policy => "security-logs-policy"
    }
  }
}